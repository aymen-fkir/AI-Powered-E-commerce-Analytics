# AI-Powered E-commerce Analytics ğŸ›ï¸ğŸ¤–

An end-to-end AI-powered analytics platform for e-commerce data processing, featuring automated sentiment analysis, KPI generation, and multi-language client support.

## ğŸ¯ Project Goal & What It Does

This project is a comprehensive data engineering solution designed to:

- **Extract** e-commerce data from various sources (Supabase storage, APIs)
- **Transform** raw data using AI-powered sentiment analysis and automated KPI calculations
- **Load** processed data into databases and storage systems for analytics
- **Provide** multi-language client interfaces (Python, Go) for data interaction
- **Generate** actionable insights from customer reviews, sales data, and user behavior

### Key Features:
- ğŸ¤– **AI-Powered Sentiment Analysis**: Uses local LLM (llama.cpp) for review classification
- ğŸ“Š **Automated KPI Generation**: Shop performance, user metrics, and time-series analytics
- ğŸ³ **Containerized Architecture**: Docker-based microservices for scalability
- ğŸ”„ **Modular ETL Pipeline**: Clean separation of extract, transform, and load operations
- ğŸŒ **Multi-Client Support**: Python and Go clients 
- ğŸ“ˆ **Real-time Processing**: Streaming and batch processing capabilities

## ğŸ“ Repository Structure

```
AI-Powered-E-commerce-Analytics/
â”œâ”€â”€ README.md                           # This file - Project overview and setup
â”œâ”€â”€ .env.example                        # Environment variables template
â”œâ”€â”€ config.yaml                         # Global configuration file
â”œâ”€â”€ docker-compose.yml                  # Multi-service orchestration
â”œâ”€â”€ .gitignore                          # Git ignore patterns
â”œâ”€â”€ 
â”œâ”€â”€ etl_pipeline/                       # ğŸ”„ Main ETL Pipeline (Python)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â””â”€â”€ etl_pipeline/
â”‚   â”‚       â”œâ”€â”€ main.py                 # Pipeline orchestrator
â”‚   â”‚       â”œâ”€â”€ models/                 # Pydantic data models
â”‚   â”‚       â”œâ”€â”€ extract/                # Data extraction module
â”‚   â”‚       â”œâ”€â”€ transform/              # AI transformation module
â”‚   â”‚       â”œâ”€â”€ load/                   # Data loading module
â”‚   â”‚       â””â”€â”€ utils/                  # Common utilities
â”‚   â”œâ”€â”€ Dockerfile                      # ETL container config
â”‚   â”œâ”€â”€ requirements.txt                # Python dependencies
â”‚   â””â”€â”€ README.md                       # ETL-specific documentation
â”‚
â”œâ”€â”€ Clients/                            # ğŸ‘¥ Client Applications
â”‚   â”œâ”€â”€ go/                             # ğŸ”· Go Client
â”‚   â”‚   â”œâ”€â”€ cmd/main.go                 # Go application entry point
â”‚   â”‚   â”œâ”€â”€ internal/                   # Internal Go packages
â”‚   â”‚   â”‚   â”œâ”€â”€ models/                 # Data structures
â”‚   â”‚   â”‚   â”œâ”€â”€ extract/                # Data extraction
â”‚   â”‚   â”‚   â”œâ”€â”€ enrichment/             # AI enrichment
â”‚   â”‚   â”‚   â””â”€â”€ load/                   # Data loading
â”‚   â”‚   â”œâ”€â”€ pkg/utils/                  # Shared utilities
â”‚   â”‚   â”œâ”€â”€ Dockerfile                  # Go client container
â”‚   â”‚   â””â”€â”€ README.md                   # Go client docs
â”‚   â”‚
â”‚   â””â”€â”€ python/                         # ğŸ Python Clients
â”‚       â”œâ”€â”€ llama_cpp_client.py         # Direct llama.cpp integration
â”‚       â””â”€â”€ ollama_client.py            # Ollama client wrapper
â”‚
â”œâ”€â”€ collect/                            # ğŸ“¡ Data Collection Service
â”‚   â”œâ”€â”€ collector.py                    # Data collection logic
â”‚   â”œâ”€â”€ Dockerfile                      # Collector container
â”‚   â””â”€â”€ requirements.txt                # Collector dependencies
â”‚
â”œâ”€â”€ llama.cpp/                          # ğŸ§  AI/LLM Infrastructure (Git Submodule)
â”‚   â”œâ”€â”€ models/                         # LLM model files (.gguf)
â”‚   â””â”€â”€ .devops/                        # Docker configs for llama.cpp
â”‚
â”œâ”€â”€ logs/                               # ğŸ“‹ Application Logs
â”‚   â””â”€â”€ http_requests.log               # HTTP request logs
â”‚
â””â”€â”€ modelfiles/                         # ğŸ“„ Model Configuration Files
    â””â”€â”€ Modelfile                       # LLM model definitions
```

### Component Overview:

#### ğŸ”„ **ETL Pipeline** (`etl_pipeline/`)
- **Purpose**: Core data processing engine
- **Technology**: Python with Polars, Pydantic, OpenAI API
- **Features**: Modular architecture, async processing, AI integration

#### ğŸ‘¥ **Client Applications** (`Clients/`)
- **Go Client**: High-performance concurrent processing
- **Python Clients**: Flexible scripting and prototyping

#### ğŸ“¡ **Data Collector** (`collect/`)
- **Purpose**: Web scraping and data ingestion
- **Features**: Rate limiting, error handling, data validation

#### ğŸ§  **AI Infrastructure** (`llama.cpp/`)
- **Purpose**: Local LLM server for sentiment analysis
- **Technology**: llama.cpp with CUDA acceleration
- **Models**: Gemma, OpenHermes, and other GGUF models

## ğŸ“‹ Prerequisites


The storage bucket follow the medalian schema as follow 

```
â”œâ”€â”€ Bronze/                       # raw data
â”‚   â”œâ”€â”€ old/                      # the raw data that have been Enriched 
â”‚   â”œâ”€â”€ new/                      # the raw data that waiting to be Enriched
â”œâ”€â”€ Silver/                       # Enriched data
â”‚   â”œâ”€â”€ processed/                # The data that have been enriched and used to generate kpis
â”‚   â”œâ”€â”€ to_process/               # The data that have been enriched and waiting to generate kpis
â”œâ”€â”€ gold/                         # final data
â”‚   
```

Create 3 Database in supabase that follow these schema :

#### Database name (user_kpis)
```
{'id': String, 'average_spent': Float64, 'positive_reviews': UInt32, 'negative_reviews': UInt32, 'likeness_score': Float64, 'normalized_likeness_score': Float64}
```

#### Database name (shop_kpis)
```
{'shop_id': String, 'average_profit': Float64, 'positive_reviews': UInt32, 'negative_reviews': UInt32, 'likeness_score': Float64, 'normalized_likeness_score': Float64})
```

#### Database name (date_kpis)
```
{'date': String, 'average_profit_per_day': Float64}
```

## ğŸš€ Getting Started

### 1. Clone the Repository
```bash
git clone --recursive https://github.com/aymen-fkir/AI-Powered-E-commerce-Analytics.git
cd AI-Powered-E-commerce-Analytics
```

### 2. Environment Setup
```bash
# Copy environment template
cp .example.env .env

# Edit with your credentials
nano .env
```

Required environment variables:
```bash
project_url=https://your-project.supabase.co
project_key=your_supabase_service_key
project_jwt_key=your-supabase-jwt-key
api_key=marcove-api-key
```

### 3. Configuration
Edit `config.yaml` to match your setup:
```yaml
# Supabase storage paths
supabase:
  bucketName: 'your-bucket-name'
  
# AI model configuration  
ETLCONFIG:
  model: 'gemma-3-1b'
  base_url: 'http://localhost:8000/v1/'
```

## ğŸ³ Running with Docker (Recommended)

### Start All Services
```bash
# Build and start all containers
docker-compose up -d

# View logs
docker-compose logs -f
```

### Run Individual Services
```bash
# LLM server only
docker-compose up llama

# ETL pipeline only  
docker-compose up etl

# Go client only
docker-compose up go-client

# Data collector only
docker-compose up collector
```

### Service Endpoints
- **LLM Server**: http://localhost:8000
- **Go Client**: http://localhost:8080  
- **ETL Pipeline**: http://localhost:5000
- **Data Collector**: http://localhost:5050

## ğŸ› ï¸ Manual Installation

### ETL Pipeline

[ETL](./etl_pipeline/README.md)

### Go Client
[CLIENT](./Clients/go/README.md)

### Data Collector
[COLLECTOR](./collect/README.md)

### llama-cpp
[LLAMA-CPP](./llama.cpp/README.md)


## ğŸ”§ Development

### Adding New Transformations
1. Create new module in `etl_pipeline/src/etl_pipeline/transform/`
2. Implement transformation logic
3. Register in main pipeline orchestrator

### Extending Client Support
1. Create new client directory in `Clients/`
2. Implement core interfaces (extract, transform, load)
3. Add Docker configuration

### Custom AI Models
1. Add model files to `llama.cpp/models/`
2. Update `config.yaml` with model configuration
3. Restart llama service

**Note**: This project demonstrates modern data engineering practices with AI integration. Perfect for learning microservices architecture, containerization, and ML operations.
you can read about my Project in This [blog](https://medium.com/@aymenfkir23/building-an-llm-powered-etl-pipeline-for-review-generation-at-scale-6af943c03feb) 
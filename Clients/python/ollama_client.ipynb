{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34dbb667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from ollama import Client,AsyncClient\n",
    "from pydantic import BaseModel, Field,conlist\n",
    "from typing import List,Literal\n",
    "import traceback\n",
    "import json\n",
    "import tqdm\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23ad21c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pl.read_parquet(\"./data/data.parquet\")\n",
    "data = data.with_columns(\n",
    "    pl.arange(1, data.height + 1).alias(\"item_id\")\n",
    ")\n",
    "category_description = data[[\"item_id\",\"description\"]].to_dict(as_series=False)\n",
    "items = [dict(zip(category_description.keys(), values)) for values in zip(*category_description.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "352dfc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constantents\n",
    "\n",
    "BATCH_SIZE=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e390e910",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemReview(BaseModel):\n",
    "    item_id: int = Field(description=\"unique identifier that is provided in the input.\",title=\"item_id\")\n",
    "    classification: str = Field(description=\"The classification of the item (e.g.,Food,cloths).\")\n",
    "    review: str = Field(description=\"A brief review of the item.\")\n",
    "\n",
    "class Response(BaseModel):\n",
    "    reviews: List[ItemReview] = Field(min_length=BATCH_SIZE,description=\"A list of classifications and reviews for the provided items.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe95614",
   "metadata": {},
   "source": [
    "## Using AsyncIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e6900e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AsyncClient(\"http://localhost:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9b653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def Chat(client,context):\n",
    "    # set tempture to 1 \n",
    "    response = await client.chat(\n",
    "        messages=context,\n",
    "        model=\"gemma-small:latest\",\n",
    "        format=Response.model_json_schema(),\n",
    "        keep_alive=20,\n",
    "        stream=False,\n",
    "        options={\n",
    "            \"num_gpu\": 30,\n",
    "            }\n",
    "        )\n",
    "    return response\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cc2cad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_text(items: list[dict]) -> str:\n",
    "    lines = []\n",
    "    for item in items:\n",
    "        line = \"\\n\".join([f\"{key} : {value}\" for key, value in item.items()])\n",
    "        lines.append(line)\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a354c5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creatPrompt(batch_items):\n",
    "    prompt_instruction = \"\"\"\n",
    "                You are a helpful assistant that classifies and reviews items.\n",
    "                \n",
    "                Each item has:\n",
    "                - \"item_id\": unique id for each item\n",
    "                - \"description\": the item's description\n",
    "                \n",
    "                Return a JSON array of objects with the following keys:\n",
    "                - \"item_id\" : same as item_id from input\n",
    "                - \"category\" : classification of item category  \n",
    "                - \"review\" : small review 1-2 phrases max\n",
    "                \"\"\"\n",
    "    \n",
    "    # Build prompt\n",
    "    number_of_items = len(batch_items)\n",
    "    batch_items = dict_to_text(batch_items)\n",
    "    context = [\n",
    "                {'role': 'system',\n",
    "                 'content': f\"{prompt_instruction}\"},\n",
    "                {\"role\": \"user\", \"content\": f\"The following {number_of_items} items need to be classified and reviewed:\\n\\n{batch_items}\"},\n",
    "            ]\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4123a1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry(result,BATCH_SIZE,batch_items):\n",
    "    if len(result[\"reviews\"]) != BATCH_SIZE:\n",
    "        print(f\"Expected {len(batch_items)} reviews, but got {len(result['reviews'])} \\nwhat i got : {result} , handeling error\")\n",
    "        \n",
    "        items_ids = [r[\"item_id\"] for r in result[\"reviews\"]] \n",
    "        rest = list(filter(lambda Id : Id not in items_ids , batch_items))\n",
    "    else:\n",
    "        rest = None\n",
    "    return rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d39580f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def batch_iter(iterable, BATCH_SIZE):\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        batch = list(islice(it, BATCH_SIZE))\n",
    "        if not batch:\n",
    "            break\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e173ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use async io to send multiple batches at the same time \n",
    "# process time : it used to be 13 days current time is 7 days to : 46.15% gain of process time\n",
    "async def main():\n",
    "    \n",
    "    all_reviews = []\n",
    "    rest = None\n",
    "    items_iter = batch_iter(items, BATCH_SIZE)  # gives batches of 100 items\n",
    "    \n",
    "    # Initialize progress bar\n",
    "    total_items = len(items)\n",
    "    pbar = tqdm.tqdm(total=total_items, desc=\"Processing items\", unit=\"items\")\n",
    "    processed_count = 0\n",
    "\n",
    "    while True:\n",
    "        if rest:\n",
    "            batches = [rest]\n",
    "            rest = None\n",
    "        else:\n",
    "            batches = [list(next(items_iter, [])) for _ in range(20)]\n",
    "            batches = [b for b in batches if b]  # remove empty\n",
    "\n",
    "        if not batches:\n",
    "            break\n",
    "\n",
    "        contexts = [creatPrompt(b) for b in batches]\n",
    "        responses = await asyncio.gather(*(Chat(client, ctx) for ctx in contexts))\n",
    "\n",
    "        for batch_items, response in zip(batches, responses):\n",
    "            try:\n",
    "                parsed = Response.model_validate_json(response.message.content)\n",
    "                result = json.loads(parsed.model_dump_json())\n",
    "                rest = retry(result, BATCH_SIZE, batch_items)\n",
    "                \n",
    "                # Only count as processed if no retry is needed\n",
    "                if rest is None:\n",
    "                    items_processed = len(batch_items)\n",
    "                    processed_count += items_processed\n",
    "                    pbar.update(items_processed)\n",
    "                    pbar.set_postfix({\"Processed\": processed_count, \"Reviews\": len(all_reviews)})\n",
    "                all_reviews.extend(result[\"reviews\"])\n",
    "                \n",
    "            except json.JSONDecodeError as jd:\n",
    "                print(f\"JSONDecodeError: {jd}\")\n",
    "            except ValueError as ve:\n",
    "                print(f\"ValueError: {ve}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected error: {e}\")\n",
    "\n",
    "    pbar.close()\n",
    "    return all_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ceb668",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing items:   0%|          | 0/1000000 [00:00<?, ?items/s]"
     ]
    }
   ],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b024dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch to llama cpp\n",
    "# write the api for it or use exisiting one \"https://llama-cpp-python.readthedocs.io/en/latest/api-reference/\"\n",
    "# make with python first than if it get  you where you want fine else\n",
    "# switch to go language for go check this out \"https://github.com/go-skynet/go-llama.cpp\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# Go Client 

## ğŸ“ Project Structure

```
â”œâ”€â”€ cmd/
â”‚   â””â”€â”€ main.go                 # Application entry point
â”œâ”€â”€ internal/                   # Private application packages
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ types.go           # Data models and structures
â”‚   â”œâ”€â”€ extract/
â”‚   â”‚   â””â”€â”€ extractor.go       # Data extraction from storage
â”‚   â”œâ”€â”€ enrichment/
â”‚   â”‚   â””â”€â”€ enricher.go        # AI-powered data enrichment
â”‚   â””â”€â”€ load/
â”‚       â””â”€â”€ loader.go          # Data loading and storage
â”œâ”€â”€ pkg/
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ utils.go           # Shared utility functions
â”œâ”€â”€ go.mod                     # Go module definition
â”œâ”€â”€ go.sum                     # Go module dependencies
```

## ğŸ—ï¸ Architecture Overview

### **Separation of Concerns**
- **Models**: All data structures and types
- **Extract**: Data extraction from Supabase storage
- **Enrichment**: AI processing using llama.cpp
- **Load**: Data merging and storage operations
- **Utils**: Common utilities and helpers


## ğŸš€ Usage

### Running the Application
```bash
# From the project root
go run cmd/main.go
```

### Building the Application
```bash
# Build binary
go build -o bin/etl-pipeline cmd/main.go

# Run binary
./bin/etl-pipeline
```

## ğŸ“¦ Package Details

### **cmd/main.go**
- Application entry point
- Orchestrates the entire ETL pipeline
- Handles client initialization
- Manages pipeline execution flow

### **internal/models**
- `Product`: Raw product data structure
- `Item`: Processed item for AI analysis
- `ItemReview`: AI-generated review structure
- `Response`: AI API response format
- `MergedResponse`: Final merged data structure

### **internal/extract**
- `Extractor`: Manages data extraction from storage
- File listing and downloading
- Data transformation and batching
- Concurrent file processing

### **internal/enrichment**
- `Enricher`: Handles AI-powered data processing
- Prompt generation and management
- OpenAI API integration
- Batch processing with retry logic

### **internal/load**
- `Loader`: Manages data loading and storage
- Data merging operations
- Batch uploading to storage
- File cleanup operations

### **pkg/utils**
- Common utility functions
- Logging with mutex protection
- Generic transformation functions
- JSON schema generation

## ğŸ”§ Configuration

The application uses environment variables for configuration:

```bash
mv .example.env .env
# add the supbase credential 
```

## ğŸ”„ Pipeline Flow

1. **Extract Phase**: Download and process files from Supabase storage
2. **Enrichment Phase**: Process data through AI models for classification and reviews
3. **Load Phase**: Merge data and upload processed results
4. **Cleanup Phase**: (Optional) Remove processed files


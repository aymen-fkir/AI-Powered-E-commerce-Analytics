
services:
  llama:
    image: local/llama.cpp:server-cuda
    build:
      context: ./llama.cpp/.devops
      dockerfile: cuda.Dockerfile
    container_name: llama-server-1
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ./llama.cpp/models:/models
      - ./logs:/logs
    command: 
      [
        "--model", "/models/gemma-3-1b-it-Q2_K.gguf",
        "--host", "0.0.0.0",
        "--port", "8000",
        "--ctx-size", "9096",
        "--temp", "0.9",
        "--n-predict", "-1",
        "--threads", "10",
        "--n-gpu-layers", "30",
        "--batch-size", "512",
        "--ubatch-size", "256",
        "--flash-attn" , "auto",
        "--cache-type-k", "f16",
        "--cache-type-v", "f16",
        "--mlock",
        "--cont-batching",
        "--parallel", "5",
        "--defrag-thold", "0.1",
        "--verbosity", "1",
        "--log-file", "/logs/http_requests.log",
        "--log-timestamps",
        "--log-prefix"
      ]
    ports:
      - "8000:8000"
    healthcheck:
      test: ["CMD-SHELL", "curl --fail http://localhost:8000 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5

  go-client:
    image: go-client:latest
    build:
      context: ./Client/go/
      dockerfile: Dockerfile
    container_name: go-client
    restart: unless-stopped
    depends_on:
      llama:
        condition: service_healthy
    environment:
      # Environment variables for the Go application
      - LLAMA_CPP_BASE_URL=http://llama:8000/v1
    volumes:
      # Mount data directory for parquet files
      - ./data:/app/data:ro
    ports:
      - "8080:8080"
